{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rugeYYiqsrlc"
   },
   "source": [
    "This file is loosely inspired by the tutorial based on:\n",
    "\n",
    "> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". Proceedings of the IEEE, 111(9) September 2023.](https://ieeexplore.ieee.org/abstract/document/10242251) </cite>\n",
    "\n",
    "Some of the code in this file is generated by ChatGPT\n",
    "\n",
    "This file makes use of the MIT-BIH Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "id": "QXZ6Tuqc9Q-l"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import wfdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "The MIT-BIH dataset contains 48 half-hour samples of ECG recordings\n",
    "\n",
    "Each recording corresponds to three raw files:\n",
    "\n",
    "1. `.hea` describes the recording's sampling rate, channels, etc.\n",
    "2. `.dat` are a sequence of 12-bit integers encoding the intensity at 360 Hz with a resolution of 1/200 mV\n",
    "    - there are two channels, but only the first one is consistent\n",
    "3. `.atr` includes the time stamp and type of each heartbeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_path = \"./raw_data/\"\n",
    "lof = []\n",
    "#for p in os.listdir(dl_path):\n",
    "#    if 'atr' in p:\n",
    "#        lof.append(int(p.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208]\n"
     ]
    }
   ],
   "source": [
    "# contains paced beats\n",
    "excludedNames = [102, 104, 107, 217]\n",
    "\n",
    "# set records to be included here\n",
    "recordNames = [208]\n",
    "for l in lof:\n",
    "    if (l not in excludedNames):\n",
    "        recordNames.append(l)\n",
    "#recordNames = [100, 101, 103, 105, 106, 108, 109, 111, 112]\n",
    "#recordNames = [100, 101]\n",
    "\n",
    "print(recordNames)\n",
    "\n",
    "# window size for each record\n",
    "windowSize = 290\n",
    "\n",
    "batch_size =  512\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "seed = 22\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "id": "lI0GbgLgpkos",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 1\n"
     ]
    }
   ],
   "source": [
    "numRec = len(recordNames)\n",
    "\n",
    "for i in range(numRec):\n",
    "    recordNames[i] = str(recordNames[i])\n",
    "\n",
    "print(\"Number of records: %d\" % numRec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the files\n",
    "\n",
    "The `.dat` file's encoding is automatically converted to double floats in mV when we read it\n",
    "\n",
    "The `.atr` is read into an object with an array of true labels and an array of time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "id": "2fhRixcspkot",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3040 3040\n"
     ]
    }
   ],
   "source": [
    "annos = [] # annotation\n",
    "records = []\n",
    "\n",
    "for i in range(numRec):\n",
    "    annos.append(wfdb.rdann(dl_path + recordNames[i], 'atr'))\n",
    "    records.append(wfdb.rdrecord(dl_path + recordNames[i]))\n",
    "\n",
    "print(len(annos[i].sample), len(annos[i].symbol))\n",
    "    \n",
    "signals = []\n",
    "for i in range(numRec):\n",
    "    # we'll worry about quantization later in the code\n",
    "    signals.append(records[i].p_signal.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heartbeat Segmentation\n",
    "\n",
    "We want each heartbeat in the records to be one data point, those data points should then be shuffled (instead of being grouped by patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N:0 S:1 V:2 F:3 Q:4; 4 is unclassifiable\n",
    "labelMap = {\n",
    "    'N':0,'L':0,'R':0,'e':0,'j':0,\n",
    "    'A':1,'a':1,'J':1,'S':1,\n",
    "    'V':2,'E':2,\n",
    "    'F':3,\n",
    "    'Q':4,'/':4,'?':4,'f':4,'P':4,'t':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of beats: 2953 290\n"
     ]
    }
   ],
   "source": [
    "beats = []\n",
    "labelChars = [] # label still in strings\n",
    "for i in range(numRec):\n",
    "    for pos, label in zip(annos[i].sample, annos[i].symbol):\n",
    "        if (label not in labelMap.keys()):\n",
    "            continue\n",
    "        winMin = pos - windowSize//2\n",
    "        winMax = pos + windowSize//2\n",
    "        if (winMin >=0 and winMax <= len(signals[i])): # this will sacrifice some signals\n",
    "            segment = signals[i][pos - windowSize//2 : pos + windowSize//2, 0] # channel 0 only\n",
    "            beats.append(segment)\n",
    "            labelChars.append(label)\n",
    "\n",
    "beats = np.stack(beats) # shape: total number of beats * window size\n",
    "print(\"Shape of beats:\", len(beats), len(beats[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AAMI suggests five categories out of the multitude of labels from MIT-BIH, we encode them into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "id": "aEtCbO6upkou",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = [labelMap.get(_x, -1) for _x in labelChars]\n",
    "\n",
    "print(set(labels))\n",
    "#plt.plot(labels)\n",
    "#plt.xlim(600, 650)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183 290 1183\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "fLabels = []\n",
    "fBeats = []\n",
    "\n",
    "'''\n",
    "for i in range(len(labels)-5):\n",
    "    if (labels[i] != 0):\n",
    "        for j in range(i - 5, i + 5):\n",
    "            fLabels.append(labels[j])\n",
    "            fBeats.append(beats[j])\n",
    "labels = fLabels\n",
    "beats = fBeats\n",
    "\n",
    "'''\n",
    "labels = labels[1770:]\n",
    "beats = beats[1770:]\n",
    "\n",
    "\n",
    "print(len(beats), len(beats[0]), len(labels))\n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    return (x - x_min) / (x_max - x_min + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative\n",
    "\n",
    "Using the derivative is sometimes helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by ChatGPT\n",
    "def dvdt(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.ndim != 3:\n",
    "        raise ValueError(f\"Expected a 3D tensor (time, batch, inputs), got {x.ndim}D.\")\n",
    "    \n",
    "    dx = x[1:] - x[:-1]\n",
    "    zero_frame = torch.zeros(1, *x.shape[1:], dtype=x.dtype, device=x.device)\n",
    "    dx = torch.cat((dx, zero_frame), dim=0)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deltas\n",
    "\n",
    "We can work with just the positive edge, or both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expecting an input of num_steps x batch x 1\n",
    "# returning either nxbx1 or nxbx2\n",
    "\n",
    "# snnTorch: if (data[n+1] - data[n] > thr): spike\n",
    "def delta_snt(data, deltaThr, negEdge=False):\n",
    "    if (not negEdge):\n",
    "        return spikegen.delta(data, threshold=deltaThr)\n",
    "    \n",
    "    spikeDataP = spikegen.delta(data, threshold=deltaThr)\n",
    "    spikeDataN = spikegen.delta(-data, threshold=deltaThr)\n",
    "    spikeData = torch.cat((spikeDataP, spikeDataN), dim=2)\n",
    "    return spikeData\n",
    "\n",
    "# alternative: if (data[n] - data[prevSpike] > thr): spike\n",
    "def delta_alt(data, deltaThr, negEdge=False):\n",
    "    if (not negEdge):\n",
    "        spikes = torch.zeros_like(data)\n",
    "        for b in range(data.size(1)):\n",
    "            prevSpike = 0\n",
    "            for n in range(data.size(0)):\n",
    "                if ((data[n, b, 0] - data[prevSpike, b, 0]) > deltaThr):\n",
    "                    spikes[n, b, 0] = 1\n",
    "                    prevSpike = n\n",
    "        return spikes\n",
    "    else:\n",
    "        # ChatGPT impl.\n",
    "        timesteps, batches, inputs = data.shape\n",
    "\n",
    "        # Initialize output spike signals with zeros\n",
    "        pos_spikes = torch.zeros_like(data, device=device)\n",
    "        neg_spikes = torch.zeros_like(data, device=device)\n",
    "    \n",
    "        # Initialize delta_sample as the first sample for each batch\n",
    "        delta_sample = data[0].clone()\n",
    "    \n",
    "        for t in range(timesteps):\n",
    "            x = data[t]\n",
    "    \n",
    "            # Positive spike detection\n",
    "            pos_mask = x > delta_sample + deltaThr\n",
    "            pos_spikes[t, pos_mask] = 1.0  # spike\n",
    "            delta_sample[pos_mask] = x[pos_mask]\n",
    "    \n",
    "            # Negative spike detection\n",
    "            neg_mask = x < delta_sample - deltaThr\n",
    "            neg_spikes[t, neg_mask] = 1.0  # spike\n",
    "            delta_sample[neg_mask] = x[neg_mask]\n",
    " \n",
    "        spikeData = torch.cat((pos_spikes, neg_spikes), dim=2)\n",
    "        return spikeData\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    \n",
    "    # Avoid division by zero if all values are equal\n",
    "    if max_val == min_val:\n",
    "        return torch.zeros_like(tensor)\n",
    "    \n",
    "    normalized = (tensor - min_val) / (max_val - min_val)\n",
    "    return normalized\n",
    "\n",
    "def normalize3(tensor, min_val, max_val):\n",
    "    # Avoid division by zero if all values are equal\n",
    "    if max_val == min_val:\n",
    "        return torch.zeros_like(tensor)\n",
    "    \n",
    "    normalized = (tensor - min_val) / (max_val - min_val)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([290, 1183, 6]) torch.Size([1183])\n"
     ]
    }
   ],
   "source": [
    "class Spike_Dataset(Dataset):\n",
    "    def __init__(self, beatsT, labelsT, deltaThr1, deltaThr2,\n",
    "                 t=torch.float32):\n",
    "        self.labels = torch.tensor(labelsT, dtype=torch.long).to(device)\n",
    "\n",
    "        data = torch.tensor(beatsT)\n",
    "        self.beats = normalize(data)\n",
    "        data = (data.unsqueeze(0)).permute(2, 1, 0)\n",
    "\n",
    "        d = normalize(data)\n",
    "        ddt = normalize(dvdt(data))\n",
    "        ddt2 = normalize(dvdt(dvdt(data)))\n",
    "        \n",
    "        spikeData0 = delta_alt(d, deltaThr1, True)\n",
    "        spikeData1 = delta_alt(ddt, deltaThr2, True)\n",
    "        spikeData2 = delta_alt(ddt2, deltaThr2, True)\n",
    "        spikeData = torch.cat((spikeData0, spikeData1, spikeData2),\n",
    "                              dim=2)\n",
    "        \n",
    "        self.spikes = spikeData.to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.spikes[:, idx, :], self.labels[idx]\n",
    "    def shapes(self):\n",
    "        print(self.spikes.shape, self.labels.shape)\n",
    "    def getBeat(self, idx):\n",
    "        return self.beats[idx]\n",
    "\n",
    "\n",
    "dataset = Spike_Dataset(beats, labels, 0.003, 0.006)\n",
    "dataset.shapes()\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                          shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhFyzySNeT_e"
   },
   "source": [
    "# Define the Network(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNN_Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepts a tensor of num_steps×batchx1\n",
    "class SNN_Baseline(nn.Module):\n",
    "    def __init__(self, beta=0.95, qbits = 8, fbits = 6):\n",
    "        super().__init__()\n",
    "        self.name = \"SNN_Baseline\"\n",
    "        \n",
    "        # Temporal Dynamics\n",
    "        self.beta = beta\n",
    "\n",
    "        # ensure all internal operations are done on fixed point\n",
    "        self.quant = False\n",
    "        # the total number of bits we can use\n",
    "        self.qbits = qbits\n",
    "        self.fbits = fbits # the number of bits representing fractions\n",
    "        \n",
    "        # Network Architecture\n",
    "        num_inputs = 2\n",
    "        num_outputs = 5 # five classes\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, 64, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=self.beta)\n",
    "        self.fc2 = nn.Linear(64, num_outputs, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=self.beta)\n",
    "\n",
    "    # quantize any tensor according to the model's rules\n",
    "    def quantize(self, x):\n",
    "        if (not self.quant):\n",
    "            return x\n",
    "        # e.g. quantize 1.1 with 4 bits incl. 2 fraction bits, 2's complement:\n",
    "        # 1. find the upper and lower bounds 0b(01.11)=1.75; 0b(10.00)=-2\n",
    "        # 2. clamp it if necessary\n",
    "        # 3. scale it by 2^3=8 => 1.1*8=8.8\n",
    "        # 4. round it to a whole number: round(8.8)=9=0b1001\n",
    "        # 5. scale back 9/8=1.125=0b(1.001) which is the closest we can do\n",
    "        # boundary check        \n",
    "        scale = 1 << self.fbits\n",
    "        \n",
    "        ub = ((1 << (self.qbits-1))-1) / scale\n",
    "        lb = ((1 << (self.qbits-1))*-1)/ scale\n",
    "        x = torch.clamp(x, lb, ub)\n",
    "        \n",
    "        x = x *scale \n",
    "        x = torch.round(x)\n",
    "        \n",
    "        x = x / scale\n",
    "\n",
    "        return x\n",
    "\n",
    "    # toggle quantized mode, irreversible for now\n",
    "    def quant_mode(self):\n",
    "        if (not self.quant):\n",
    "            self.quant = True\n",
    "            self.fc1.weight.data = self.quantize(self.fc1.weight.data)\n",
    "            self.fc2.weight.data = self.quantize(self.fc2.weight.data)\n",
    "\n",
    "    # can report the hidden value for logging\n",
    "    def forward(self, x, num_steps, hidden=False):\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        mem1_rec = []\n",
    "        \n",
    "        # Record the final layer\n",
    "        spkx_rec = []\n",
    "        memx_rec = []\n",
    "\n",
    "        # Feed in the spikes of the entire batch at a given time step in parallel\n",
    "        for step in range(num_steps):\n",
    "            # the weights are already quant'd, no need to quant spikes\n",
    "            cur1 = self.fc1(x[step])\n",
    "            # the resulting multiple is still quant'd\n",
    "            cur1 = torch.clamp(cur1, 0, None) # room for optimisation for quant\n",
    "\n",
    "            # potential quant bug: depending on how the currents are summed up they\n",
    "            # will have the same precision, but can exceed the upper bound\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            # consistently quant mem1 after use\n",
    "            mem1 = self.quantize(mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            cur2 = torch.clamp(cur2, 0, None)\n",
    "            \n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            # consistently quant mem2 after use\n",
    "            mem2 = self.quantize(mem2)\n",
    "            \n",
    "            spkx_rec.append(spk2)\n",
    "            memx_rec.append(mem2)\n",
    "\n",
    "            \n",
    "            if (hidden):\n",
    "                mem1_rec.append(mem1)\n",
    "        \n",
    "        if (hidden):\n",
    "            mem1_rec = torch.stack(mem1_rec, dim=0)\n",
    "        else:\n",
    "            mem1_rec = None\n",
    "\n",
    "        return (torch.stack(spkx_rec, dim=0),\n",
    "                torch.stack(memx_rec, dim=0),\n",
    "                mem1_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNN_Big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepts a tensor of num_steps×batchx1\n",
    "class SNN_Big(nn.Module):\n",
    "    def __init__(self, beta=0.95, qbits = 8, fbits = 6, num_inputs_p = 2):\n",
    "        super().__init__()\n",
    "        self.name = \"SNN_Big\"\n",
    "        \n",
    "        # Temporal Dynamics\n",
    "        self.beta = beta\n",
    "\n",
    "        # ensure all internal operations are done on fixed point\n",
    "        self.quant = False\n",
    "        # the total number of bits we can use\n",
    "        self.qbits = qbits\n",
    "        self.fbits = fbits # the number of bits representing fractions\n",
    "        \n",
    "        # Network Architecture\n",
    "        num_inputs = num_inputs_p\n",
    "        num_outputs = 5 # five classes\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, 64, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=self.beta, reset_mechanism='zero')\n",
    "        self.fc2 = nn.Linear(64, 128, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=self.beta, reset_mechanism='zero')\n",
    "        self.fc3 = nn.Linear(128, 64, bias=False)\n",
    "        self.lif3 = snn.Leaky(beta=self.beta, reset_mechanism='zero')\n",
    "        self.fc4 = nn.Linear(64, num_outputs, bias=False)\n",
    "        self.lif4 = snn.Leaky(beta=self.beta, reset_mechanism='zero')\n",
    "\n",
    "\n",
    "    # quantize any tensor according to the model's rules\n",
    "    def quantize(self, x):\n",
    "        if (not self.quant):\n",
    "            return x\n",
    "        # e.g. quantize 1.1 with 4 bits incl. 2 fraction bits, 2's complement:\n",
    "        # 1. find the upper and lower bounds 0b(01.11)=1.75; 0b(10.00)=-2\n",
    "        # 2. clamp it if necessary\n",
    "        # 3. scale it by 2^3=8 => 1.1*8=8.8\n",
    "        # 4. round it to a whole number: round(8.8)=9=0b1001\n",
    "        # 5. scale back 9/8=1.125=0b(1.001) which is the closest we can do\n",
    "        # boundary check        \n",
    "        scale = 1 << self.fbits\n",
    "        \n",
    "        ub = ((1 << (self.qbits-1))-1) / scale\n",
    "        lb = ((1 << (self.qbits-1))*-1)/ scale\n",
    "        x = torch.clamp(x, lb, ub)\n",
    "        \n",
    "        x = x *scale \n",
    "        x = torch.round(x)\n",
    "        \n",
    "        x = x / scale\n",
    "\n",
    "        return x\n",
    "\n",
    "    # toggle quantized mode, irreversible for now\n",
    "    def quant_mode(self):\n",
    "        if (not self.quant):\n",
    "            self.quant = True\n",
    "            self.fc1.weight.data = self.quantize(self.fc1.weight.data)\n",
    "            self.fc2.weight.data = self.quantize(self.fc2.weight.data)\n",
    "            self.fc3.weight.data = self.quantize(self.fc3.weight.data)\n",
    "            self.fc4.weight.data = self.quantize(self.fc4.weight.data)\n",
    "\n",
    "    # can report the hidden value for logging\n",
    "    def forward(self, x, num_steps, hidden=False):\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "\n",
    "        mem1_rec = []\n",
    "        \n",
    "        # Record the final layer\n",
    "        spkx_rec = []\n",
    "        memx_rec = []\n",
    "\n",
    "        # Feed in the spikes of the entire batch at a given time step in parallel\n",
    "        for step in range(num_steps):\n",
    "            # the weights are already quant'd, no need to quant spikes\n",
    "            cur1 = self.fc1(x[step])\n",
    "            # the resulting multiple is still quant'd\n",
    "            #cur1 = torch.clamp(cur1, 0, None) # room for optimisation for quant\n",
    "            # potential quant bug: depending on how the currents are summed up they\n",
    "            # will have the same precision, but can exceed the upper bound\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            # consistently quant mem1 after use\n",
    "            mem1 = self.quantize(mem1)\n",
    "            \n",
    "            cur2 = self.fc2(spk1)\n",
    "            #cur2 = torch.clamp(cur2, 0, None)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            # consistently quant mem2 after use\n",
    "            mem2 = self.quantize(mem2)\n",
    "            \n",
    "            cur3 = self.fc3(spk2)\n",
    "            #cur3 = torch.clamp(cur3, 0, None)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            # consistently quant mem3 after use\n",
    "            mem3 = self.quantize(mem3)\n",
    "\n",
    "            cur4 = self.fc4(spk3)\n",
    "            #cur4 = torch.clamp(cur4, 0, None)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "            # consistently quant mem4 after use\n",
    "            mem4 = self.quantize(mem4)\n",
    "            \n",
    "            spkx_rec.append(spk4)\n",
    "            memx_rec.append(mem4)\n",
    "\n",
    "            \n",
    "            if (hidden):\n",
    "                mem1_rec.append(mem1)\n",
    "        \n",
    "        if (hidden):\n",
    "            mem1_rec = torch.stack(mem1_rec, dim=0)\n",
    "        else:\n",
    "            mem1_rec = None\n",
    "\n",
    "        return (torch.stack(spkx_rec, dim=0),\n",
    "                torch.stack(memx_rec, dim=0),\n",
    "                mem1_rec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "editable": true,
    "id": "-IxcnBAxpkoy",
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tensor(1, device='cuda:0')\n",
      "{0: 645, 1: 1, 2: 331, 3: 47, 4: 0}\n",
      "{0: 753, 1: 1, 2: 333, 3: 94, 4: 2}\n"
     ]
    }
   ],
   "source": [
    "net = SNN_Big(0.9375, 12, 4, 6).to(device)\n",
    "net.load_state_dict(torch.load(\"cps/net_premier.pth\"))\n",
    "\n",
    "trues = {0:0, 1:0, 2:0, 3:0, 4:0}\n",
    "totals = {0:0, 1:0, 2:0, 3:0, 4:0}\n",
    "c = 0\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    net.quant_mode()\n",
    "    counter = 0\n",
    "    for data, targets in loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        data = data.permute(1, 0, 2)\n",
    "        # spikeData should be in (num_steps)xbatchxinputs\n",
    "\n",
    "        # forward pass\n",
    "        test_spk, mem_rec, mem1_rec = net(data, 290, True)\n",
    "\n",
    "        # sample plots\n",
    "        if (random.randint(0, 10000) < 800) and False:\n",
    "            pass\n",
    "\n",
    "    \n",
    "        #print(test_spk.sum(dim=0))\n",
    "        # calculate total accuracy\n",
    "        _, predicted = test_spk.sum(dim=0).max(1)\n",
    "\n",
    "        for i in range(len(predicted)):\n",
    "            trues[int(targets[i])] += int(predicted[i] == targets[i])\n",
    "            totals[int(targets[i])] += 1\n",
    "            c += 1\n",
    "            if (targets[i] == 1):\n",
    "                print(c, predicted[i])\n",
    "\n",
    "print(trues)\n",
    "print(totals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "snntorch_tutorial_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
